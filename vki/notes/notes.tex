\documentclass{article}

\include{zachs_macros}

\usepackage{cleveref}
\usepackage{listings}
\usepackage[top=1in, bottom=1in, left=0.8in, right=0.8in]{geometry}
\usepackage{amssymb} % \varnothing
\usepackage{cancel} % \cancel{}

\definecolor{palegreen}{HTML}{01B7A8}
\definecolor{palered}{HTML}{C52E35}

\title{The Curse of Dimensionality: Problems and Strategies}
\author{Zachary del Rosario}

\begin{document}
\maketitle

These notes support a lecture I gave at VKI on 2018-10-17 on the ``Curse of
Dimensionality.''\footnote{The source for these notes are available in a public
  GitHub repository: https://github.com/zdelrosario/hyperspace. Feel free to
  send me a pull request if you have a suggestion, or think something here is in
  error.} This was a part of lecture series STO-AVT-236: Uncertainty
Quantification in Computational Fluid Dynamics. While the ostensible focus was
on UQ, I choose to cover issues of dimensionality as they strongly impact
efforts to quantify uncertainty. Usually when doing UQ, we care about
dimensionality insofar as it makes our lives challenging; however, towards the
end of these notes I'll point to some areas where in addressing dimensionality,
we may actually learn a bit more of the underlying physics.

\section{Introduction}
% --------------------------------------------------
First, an introduction by way of example: Let's imagine we have a function $f$
of $d$ variables $\vx\in[0,1]^d$, and we seek to carry out a parameter study. In
this study, we will choose three points per dimension, in order to `evenly'
sample the space. The number of points required grows exponentially,
as visually depicted in Figure \ref{fig:samples}.

\begin{figure}[!ht]
  \centering
  \begin{minipage}{0.45\textwidth}
    \includegraphics[width=0.9\textwidth]{../../images/points1}
  \end{minipage} %
  \begin{minipage}{0.45\textwidth}
    \includegraphics[width=0.9\textwidth]{../../images/points2}
  \end{minipage}
  \caption{Visual depiction of samples for $d=1$ and $d=2$.}
  \label{fig:samples}
\end{figure}

\emph{Exponential} scaling is bad, but some of the badness can be lost in
abstraction. If the one-dimensional case took $10$ seconds to execute, Table
\ref{tab:exponential} give execution times for some values of $d$. Note that by
$d=20$, the total execution time for our `simple' parameter study is already 240
times the age of the universe. Clearly, this is \emph{quite bad}.

\begin{table}[!ht]
  \centering
  \begin{tabular}{r|r|l}
  \hline
  Dimension & Time (sec) & Comparison\\
  \hline
  1 & $10^{1}$ & Ten seconds\\
  \hline
  5 & $10^{5}$ & One Day\\
  \hline
  10 & $10^{10}$ & Eleven generations\\
  \hline
  18 & $10^{18}$ & Age of Universe\\
  \hline
  20 & $10^{20}$ & 230 x AoU\\
  \hline
  \end{tabular}
  \caption{Execution times for example parameter study. Punchline: exponential
    scaling is \emph{very, very bad}.}
  \label{tab:exponential}
\end{table}

This exponential scaling is one manifestation of \emph{the curse of
  dimensionality}.

\section{Curse of Dimensionality}
% --------------------------------------------------
The \textbf{curse of dimensionality}\footnote{Coined by Richard Bellman in the
  context of `exhaustive optimization'.} is the general phenomena of cost
scaling exponentially with dimension. This is actually a somewhat \emph{vague}
term, as dimensionality manifests differently across
contexts.\cite{donoho2000high} Some (academic) examples include:

\bigskip\noindent\underline{Integration:} Employing a tensor quadrature grid (as
visualized in Fig. \ref{fig:samples}) for quadrature directly leads to
exponential cost.

\bigskip\noindent\underline{Sampling:} Sampling according to a tensor design
leads to exponential cost. Note, however, that the Monte Carlo algorithm
(sampling from a density to approximate expectations) is \emph{not} cursed by
dimensionality, as Monte Carlo is dimension
independent.\cite{owen2013montecarlo}

\bigskip\noindent\underline{Machine Learning:} (Requirement of large data)

\bigskip\noindent\underline{Inference:} \textcolor{red}{TODO:}

\bigskip\noindent\underline{Distance:} \textcolor{red}{TODO:} Failure
of nearest neighbors.

\bigskip\noindent\underline{Big Data:} In 2000, David Donoho gave a talk on the
curse of dimensionality as it relates to data analysis.\cite{donoho2000high}
He noted that in industry,

\begin{quote}
  The trend today is towards more observations \emph{but even more so},
  \textcolor{palered}{to radically larger numbers of variables} â€“ voracious,
  automatic, systematic collection of hyper-informative detail about each
  observed instance.\footnote{Emphasis mine}
\end{quote}

Statisticians usually consider data in terms of a matrix; with rows representing
$n$ observations, and columns representing $d$ variables. Dimensionality is thus
`one-half' of the bigness of data.

Hopefully one of the contexts above is relevant to you. Assuming this is the
case, then dimensionality is \emph{your} problem! We've seen one way
dimensionality affects a particular context (tensor-product sampling designs),
but why does dimensionality affect all these \emph{different} contexts? To build
some intuition, let's take a look at some properties of high-dimensional
geometry.

\subsection{High-Dimensional Geometry}
% -------------------------
Why is dimensionality so challenging? Here we talk about a few properties of
high-dimensional geometry. The intent is to (1) underscore that high-dimensional
spaces have `surprising' behavior, and (2) build some intuition for
dimensionality, with an aim towards challenges and potential solutions.

\subsubsection{Fact 1}
The hypersphere has vanishing interior.

\bigskip
In low dimensions, it can be easy to think of a uniform distribution -- really
just the volume of a solid -- as being `even' across the space. While this is
formally what a uniform distribution is, our geometric intuition for what that
means can start to break down in higher dimensions. As an example, let's
consider the volume integral for a unit \emph{hypersphere}; the set of all
points $\cH=\{\vx\in\R{d}\,|\,\|\vx\|\leq 1\}$, given in spherical coordinates
in \eqref{eq:hypersphere-volume}.

\begin{equation} \begin{aligned} \label{eq:hypersphere-volume}
    HV &= \int\cdots\int \textcolor{red}{r^{d-1}}\,
         T(\varphi_{1},\dots,\varphi_{d-1})\,
         dr d\varphi_{1}\cdots d\varphi_{d-1}
\end{aligned} \end{equation}

\noindent The integrand of \eqref{eq:hypersphere-volume} can be understood as a
sort of `volume density' -- the quantity we will integrate to compute the
volume. In areas where this integrand is large we see more volume contributed,
while in areas where this is small, relatively little volume is contributed.
Expressing this integral in spherical coordinates allows us to focus on how
volume contributions change along the radial coordinate $r$.

Note that the exponent in $r^{d-1}$ already points to some sort of `exponential
badness' we might expect from dimensionality. This factor in the integrand (Fig.
\ref{fig:hypervolume}) shows that the integrand tends to grow as one moves from
the origin towards the edge of the hypersphere. However, this growth becomes
distorted in higher dimensions; at $d=50$ the integrand is nearly zero until
about $r>0.9$, which means there is essentially no volume contributed until one
reaches the edge of the sphere. It is in this sense that we say the hypersphere
is empty.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.65\textwidth]{../../images/surface_density}
  \caption{Radius-dependent portion of the integrand for a (unit) hypersphere
    volume calculation. Note that for higher dimensions, nearly all of the
    integrand is concentrated near the edge of $r=1$. This demonstrates that the
    hypersphere is empty -- that all of its volume is concentrated at its edge.}
  \label{fig:hypervolume}
\end{figure}

Practically, this means that if we sample uniformly from a hypersphere, then in
high dimensions, we will surely `miss' the interior. If our intent was to
understand the relation between input and response, then this is a region of a
function's input that we will not learn about. Depending on the context, this
may (or may not!) be an issue.

\subsubsection{Fact 2} All the volume in a hypersphere is near
\emph{the} equator.

\bigskip
Upfront, this is a bit of a wordgame; there are actually \emph{many} equators on
any sphere. On Earth, we define a single equator, but one can actually think of
an infinite family of equators, each enclosed by a great circle (Fig.
\ref{fig:great-circle}). These equators are formed by intersecting a
3-dimensional sphere (a `3-sphere') with a 2-dimensional plane that passes
through the sphere's origin, forming a 2-dimensional object. Since there are
infinitely many such planes, there are infinitely many equators.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.65\textwidth]{../../images/great_circle}
  \caption{Cartoon depicting the boundary of a great circle for a 3-sphere. The
    red curve depicts a great circle; which encloses an equator. Thus, for the
    3-sphere, an equator is a 2-dimensional object. In higher dimensions $d$,
    the analog of an equator is a $d-1$-dimensional object. From Wikimedia
    Commons, CC P.wormer.}
  \label{fig:great-circle}
\end{figure}

In higher dimensions $d$, an equator is always a $d-1$-dimensional object. Thus,
as $d$ grows, an equator neglects proportionally less with regard to the full
dimensionality of the space. Thus, if we were to sample points uniform randomly
from within the $d$-dimensional sphere, this `neglected' dimension would
contribute less in higher dimensions. The result is that the volume -- which can
be linked to the uniform density in the hypersphere -- concentrates towards
\emph{any} chosen equator. We can see this by picking an arbitrary equator,
sampling at random, and computing the histogram of angles formed by the vector
between drawn points, the origin, and the chosen equator. Figure
\ref{fig:equator} shows results from this computer experiment: The realized
angles concentrate towards zero as $d$ increases.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.65\textwidth]{../../images/equator}
  \caption{Realized angles away from a chosen equator for a $d$ dimensional
    hypersphere, estimated as empirical densities. This illustrates that, as
    $d\to\infty$, most points in the hypersphere (thus the volume) are
    concentrated near an equator.}
  \label{fig:equator}
\end{figure}

Similar to above, in high dimensions we will `miss' some \emph{directions} when
sampling uniformly from the hypersphere. As noted with Fact 1, this may or may
not be an issue, depending on the context.

Facts 1 and 2 really are pure geometric properties. I suspect that if we could
visualize higher-dimensional spaces, these `surprising facts' would be obvious.
The next fact relies on some properties\footnote{concentration of measure} of
random variables, and so relies on a bit more than geometry alone.

\clearpage
\subsubsection{Fact 3}
Random projections preserve pairwise distances.

\bigskip
First, we'll illustrate this fact with an example, then describe the theorem
that backs it up. I use some gene expression data from the UC Irvine machine
learning databse\cite{blake1998uci} for this example.\footnote{The specific data
  source is not terribly important here; the point is that the data are not
  `cooked' for demonstration purposes.} I intersperse R code with text; the aim
is to describe in words what's going on, and give the explicit machine
instructions that will make it happen.\footnote{This too is on the GitHub repo:
  https://github.com/zdelrosario/hyperspace} The data are loaded into the
variable \lstinline{mat_data}; the data have dimensions

\begin{lstlisting}
## Gene data from UCI
dim(mat_data)
#       Observations,  Dimensions
> [1]   801,           20531
\end{lstlisting}

We select a random projection of dimensionality \lstinline{k} via some
mysterious, to-be-described procedure, based on a user-selected error tolerance
\lstinline{eps}.

\begin{lstlisting}
## Make reproducible
set.seed(101)
## Calculate dimension
C <- 1                             # Over-samping factor
n <- dim(mat_data)[1]              # Observations
d <- dim(mat_data)[2]              # Dimensionality

k <- C * ceiling(log(n) / eps ^ 2) # J-L dimension
k
> 1338 (6.5% of 20531)
\end{lstlisting}

This mysterious procedure has suggested a projection of dimensionality $1338$,
just $6.5\%$ of $20531$.

A bonus fact is that, in high dimensions, random vectors drawn from an isotonic
gaussian\footnote{A gaussian possessing a diagonal covariance matrix} will
essentially be orthogonal.\cite{ailon2009fast} We can use this fact to easily
construct a random projection of the desired size.

\begin{lstlisting}
## Random `projection`
P <-
  rnorm(d * k, sd = 1 / d) %>%
  matrix(nrow = d, ncol = k)
## Project the data
mat_proj <-
  mat_data %*% P
\end{lstlisting}

When we project to a smaller dimensionality, we tend to decrease the average
distance between points. To have any hope of preserving pairwise distances, we'd
better account for this fact.

\begin{lstlisting}
## Match the original average distance; Subsample for speed
Ind <- sample(x = 1:n, size = 500)

D_sub_orig <-
  mat_data[Ind, ] %>%
  dist()

D_sub_fix <-
  mat_proj[Ind, ] %>%
  dist()

factor <-
  mean(D_sub_orig, na.rm = TRUE) / mean(D_sub_fix, na.rm = TRUE)

mat_proj <-
  mat_proj * factor
\end{lstlisting}

The claim is that the projected data \lstinline{mat_proj} possesses the same
pairwise distances $\|\vu-\vv\|$ as \lstinline{mat_data}, to within the chosen
tolerance \lstinline{eps}; let's check if that is the case.

\begin{lstlisting}
## Re-compute distances with proper scaling
Ind <- sample(x = 1:n, size = 500) # New draw

D_sub_orig <-
  mat_data[Ind, ] %>%
  dist()

D_sub_proj <-
  mat_proj[Ind, ] %>%
  dist()

## Compute quantiles of discrepancy
R_diff <- (D_sub_proj - D_sub_orig) / D_sub_orig
qt <- quantile(R_diff)
## qt
sprintf(
  "%5.3f%% %5.3f%% %5.3f%% %5.3f%% %5.3f%%",
  qt[1] * 100,
  qt[2] * 100,
  qt[3] * 100,
  qt[4] * 100,
  qt[5] * 100
)
>      0%     25%    50%    75%   100%
> -7.784% -1.193% 0.082% 1.344% 8.139%
\end{lstlisting}

In this case, the realized distortion is indeed within $\pm10\%$. This is a bit
mysterious -- by \emph{randomly} throwing out information, we were able to
preserve a great deal of structure in the data. The
\textbf{Johnson-Lindenstrauss lemma} (J-L) shows that this is no accident, but
rather a general property of high-dimensional spaces.

\bigskip
\underline{Lemma:} (Johnson-Lindenstrauss) For any $0<\epsilon<1$ and
$n\in\mathbb{Z}_{>0}$, let $k\in\mathbb{Z}_{>0}$ such that

\begin{equation} \label{eq:jl-bound}
  k \geq C \frac{\log(n)}{\epsilon^2},
\end{equation}

\noindent then for all sets of points $V\subset\R{d}$, there is a projection
$P_k:\R{d}\to\R{k}$ with $\alpha\in\R{}_{>0}$ such that, for all $u,v\in V$, we
have

\begin{equation} \label{eq:jl-distance}
  (1 - \epsilon)\|\vu - \vv\|^2 \leq \alpha\|P_k(\vu) - P_k(\vv)\|^2 \leq %
  (1 + \epsilon)\|\vu - \vv\|^2.
\end{equation}

\noindent Further, this projection can be found in polynomial time.

\bigskip
Numerous proofs of this statement\footnote{note that I've modified the statement
  above to emphasize certain elements, in the interest of aiding explanation.
  The `true' J-L lemma is stated slightly differently; I've made $\alpha$
  explicit in the statement above} exist, for an example see Dasgupta and
Gupta.\cite{dasgupta2003elementary}

Let's first try to understand the statement above. \Cref{eq:jl-distance} is a
statement about pairwise distances $\|\vu-\vv\|$ and their projected versions
$\|P_k(\vu) - P_k(\vv)\|$. J-L states that \emph{all} points in a given (data)
set satisfy this condition. Precisely, the projected pairwise distances can only
be distorted at most by a factor of $(1\pm\epsilon)$.

Note that the projected pairwise distances are multiplied by a positive value
$\alpha$. This is because projections reduce the \emph{average} distance between
points, as illustrated in Figure \ref{fig:jl-avg-distance}. If we want any hope
of matching pairwise distances, we have to account for this fact. I do this very
simply in the code above by computing $\alpha$ from the ratio of average
distances between the original and projected data -- we then restore by
multiplying by alpha.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.65\textwidth]{../../images/dim_proj1}
  \caption{Example points in 3d space, projected to lower dimensions. Note the
    average distance between points is reduced with successively smaller
    projections. This is accounted for in \eqref{eq:jl-distance} by the $\alpha$
    factor, which corrects for these mean-distance effects.}
  \label{fig:jl-avg-distance}
\end{figure}

\Cref{eq:jl-bound} provides a target dimensionality based on provided
quantities. Provided the number of observations $n$ and desired error tolerance
$\epsilon$, J-L suggests a minimum projection dimension $k$. Note that
\emph{dimensionality is absent} from this bound. The original dimensionality $d$
can be arbitrarily large; given the same $n$ and $\epsilon$, the bound on
projection dimension will be the same.

This dimensionality $k$ is something we'll refer to -- in a somewhat
\emph{vague} sense -- as \emph{intrinsic dimensionality}. Within a space of
dimension $d$, there is low-dimensional structure of size $k$ (the mapping
$\alpha P_k(\cdot)$) that preserves information of interest, here pairwise
distances. This points forward to a general strategy for lifting the curse of
dimensionality -- if we can identify low-dimensional structure that preserves
the properties we care about, then we can reduce the effective dimensionality
from $d$ to $k$, and (potentially) render tractable the problems we want to
solve. This is the idea behind \emph{dimension reduction}.

\section{Dimension Reduction}
% --------------------------------------------------

\textbf{Dimension reduction} is the general strategy of identifying some form of
low-dimensional structure within a problem. Since there are many different
problem settings and objectives -- thus many \emph{kinds} of structure -- we
introduce a simple taxonomy to help distinguish different flavors of dimension
reduction.

\subsection{A Taxonomy}
% -------------------------
We will note three flavors of dimension reduction, broadly categorized by the
interpretation of $\R{d}$. Note that many of the techniques we will introduce
are applied across different contexts -- the mathematics does not appreciably
change in these different settings. However, the \emph{interpretation} of
mathematics changes across settings; if we seek to understand how the math
applies to our particular setting -- and if we seek to understand work by other
researchers in different fields -- then this difference in interpretation is
meaningful.

For instance, the singular value decomposition (SVD) shows up under different
names in different communities: the Karhunen-Loeve expansion (KLE), the proper
orthogonal decomposition (POD), principal component analysis (PCA), etc.
Mathematically, there is little difference between these ideas. However, a
person using the term KLE will generally be thinking of a different kind of
problem than someone speaking of PCA. Further, different properties of the SVD
are relevant in these different contexts. Both the communication aims and
technical details are conveyed by the different terminology.

We will describe the taxonomy of different settings, then focus on the
\emph{input-space} setting to give a bit more detail.

\bigskip\noindent A \emph{generic-space} is $\R{d}$ with no special assumptions.
Techniques like the Johnson-Lindenstrauss lemma and principal component
analysis\cite{fodor2002survey} are used in this context.

\bigskip\noindent An \emph{output-space} is $\R{d}$ understood as the output of
some (dynamical) system; the $d$ entries of a vector from $\R{d}$ could
represent a time-varying signal (e.g. a speech sample), the pointwise entries of
some field (e.g. a mean channel-flow field), or some combination (e.g. a
time-varying flow field). The techniques of compressed sensing
(CS)\cite{candes2008introduction} and the proper orthogonal
decomposition\cite{lumley2007} are often used in this context.

\bigskip\noindent An \emph{input-space} is $\R{d}$ understood as the input space
to some function $f(\vx)$. Compressed sensing is also used in this
context,\cite{tang2014subsampled} but other approaches are a bit more common. We
will discuss these below, under additional broad categorizations.

\subsection{Input-space: Running Example}
% -------------------------
To explore input-space dimension reduction techniques, we first introduce the
simple illustrative problem of rough pipe flow, like that originally studied by
Osborne Reynolds.\cite{reynolds1883}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.65\textwidth]{../../images/pipe_diagram}
  \caption{Schematic for example pipe flow problem. A viscous fluid is pumped
    through a rough pipe. Our output quantity of interest is the (normalized)
    pressure loss -- a measure of the effort required to pump the fluid. The
    inputs are the fluid properties and pipe geometry.}
  \label{fig:pipe_diagram}
\end{figure}

We will categorize the following input-space reduction techniques into
\emph{subset reduction} and \emph{subspace reduction} approaches.

\subsection{Subset Reduction}
% -------------------------
When pursuing a \emph{subset reduction}, we aim to select a subset
$\{x_i\}_{i\in I}$ of the full set of variables $\{x_i\}_{i=1,\dots,d}$. The
rationale is as follows: Suppose our qoi had some complementary subsets of
\emph{active} $\{x_i\}_{i\in I_A}$ and \emph{inactive} $\{x_i\}_{i\in I_I}$
variables, such that $I_A\cup I_I = \{1,\dots,d\}$ and $I_A\cap
I_I=\varnothing$. Further suppose this split had the property that, for all
$x_i\neq x'_i$ with $i\in I_I$ and $x_j=x'_j$ with $j\in I_A$, we have

\begin{equation} \label{eq:inactive}
  f(\vx) = f(\vx').
\end{equation}

\noindent In words, the inactive variables do not contribute to changes in the
value of $f$. In this (admittedly contrived) setting, one can safely set the
inactive variables $\{x_i\}_{i\in I_I}$ to fixed, nominal values, and carry out
further studies on the active variables -- this reduces dimensionality to $k =
|I_A|$.\footnote{Where I use $|\cdot|$ to denote set cardinality.}

Of course, one can ask why these supposedly inactive variables were in the
function to start. Practically, the condition \eqref{eq:inactive} does not
frequently occur in practice; however, this condition is approximately met in
many practical applications. The precise definition of `approximately' results
in different subset reduction techniques. We will discuss two: Morris screening
and Sobol' indices.

\subsubsection{Morris screening}
\emph{Morris screening} can be thought of as studying contributions of variables
to the qoi through the mean and variance of the derivative. Ralph Smith's
textbook \cite{smith2013uncertainty} goes through this procedure in gory detail;
these notes will give only a brief introduction, as we'll focus more on Sobol'
indices. Morris screening considers statistics of the \emph{elementary effects},
defined by

\begin{equation}\label{eq:elem-effect}
  d_{i}(\vx_j) = \frac{f(\vx_j+\Delta \ve_i) - f(\vx_j)}{\Delta},
\end{equation}

\noindent where \(\ve_i\) is the i-th standard basis vector,\footnote{The vector with
all zeros, except for a one in the i-th entry} and \(\Delta\) is a large stepsize.
Note that \eqref{eq:elem-effect} is simply a first-order approximation of the
derivative; however, since \(\Delta\) is relatively large, this is a coarse
approximation to the derivative. These elementary effects are used to construct
global sensitivity measures

\begin{equation}\begin{aligned}
  \mu_i^* &= \frac{1}{r}\sum_{j=1}^r |d_{i}(\vx_j)|, \\
  \sigma_i^2 &= \frac{1}{r-1}\sum_{j=1}^r(d_{i}(\vx_j)-\mu_i)^2, \\
  \mu_i &= \frac{1}{r}\sum_{j=1}^rd_{i}(\vx_j).
\end{aligned}\end{equation}

This procedure is called \emph{screening} because it allows us to determine
whether individual parameters are unimportant, but does not quantify relative
variable importance. To determine relative importance, we can turn to Sobol'
indices. However, Morris screening is a relatively cheap procedure, and thus is
a good tool of which to be aware.

\subsubsection{Sobol' indices}
\emph{Sobol' indices} quantify importance of variables by attributing the
variance in the qoi to different inputs. They are a useful, well-studied, and
commonly-used technique for subset reduction, so we will spend some time
discussing them in detail: I learned this topic through the primer by Saltelli
et al.\cite{saltelli2004sensitivity}

Sobol' indices require a random variable interpretation of our quantity of
interest, so we'll introduce some notation used throughout the section. Let
\(\mX\sim\rho\) be our random input variables, distributed according to a joint
density \(\rho\) over our input space \(\Omega\).\footnote{Sobol' indices also
  have some properties that rely on independence of the inputs. We will return
  to this point later.} Then our output qoi is also a random variable
\(Y=f(\mX)\). We denote by \(\E[Y] = \int f(\vx)\rho(\vx)d\vx\) the expectation,
and by \(\V[Y]=\E[(Y-\E[Y])^2]\) the variance. Note that the following useful
identity involving the variance holds

\begin{equation}
  \V[Y] = \E[Y^2] - \E[Y]^2.
\end{equation}

We denote conditioning by a vertical bar; this corresponds to holding particular
random inputs fixed in value, and leaving them out of the integral for a
particular expectation. For example, if \(Y=f(X_1,X_2,X_3)\), then

\begin{equation}
  \E_{X_2,X_3}[Y|X_1=x_1] = \int f(x_1,x_2,x_3)\rho(x_1,x_2,x_3)dx_2dx_2,
\end{equation}

\noindent where we use subscripts to make explicit the integration variables.
Note that \(\E_{X_2,X_3}[Y|X_1]\) is still a random variable\footnote{when it
  lacks the equality \(X_1=x_1\)}, due to the variability arising from \(X_1\).
We will use conditional variance to decompose the total variance \(\V[Y]\)
according to different input contributions. This will involve expressions of the
sort \(\E_{X_2,X_2}[\V_{X_1}[Y|X_1]]\).\footnote{which is no longer a random
  variable, as we have integrated out all of the variables}

In what follows, we will drop the variable subscripts, as they tend to render
expressions indecipherable. The same information is implied by the conditioning;
the expression \(\V[Y|X_1]\) carries out the integration keeping \(X_1\) fixed,
which is then integrated out in the expression \(\E[\V[Y|X_1]]\).

To construct the Sobol' indices, we will derive a particular variance
decomposition. First, we will introduce some notation to help keep track of
variable subsets. Let \(\vu\) be an \emph{index set}; that is
\(\vu\subseteq\{1,\dots,d\}\). We will use \(\vu\) to denote subsets of the
input variables; in the example above, we used \(\vu=\{2,3\}\) to give us
\(\mX_{\vu}=\{X_2,X_3\}\). Let \(-\vu\) be the set complementary to \(\vu\) with
respect to \(\{1,\dots,d\}\); in the example above \(-\vu=\{1\}\).

Next, we will prove a simple identity, which will allow us to attribute the
variance \(\V[Y]\) to different inputs. Note that for any index set \(\vu\), we have

\begin{equation}\begin{aligned}\label{eq:var-decomp}
  \E[\V[Y|\mX_{\vu}]] + \V[\E[Y|\mX_{\vu}]] &= %
  \E\left[\E[Y^2|\mX_{\vu}] - \cancel{\E[Y|\mX_{\vu}]^2 }\right] \\
  &+ \cancel{\E\left[\E[Y|\mX_{\vu}]^2\right]} - \E\left[\E[Y|\mX_{\vu}]\right]^2, \\
  &= \E[Y^2] - \E[Y]^2, \\
  &= \V[Y^2].
\end{aligned}\end{equation}

\noindent \Cref{eq:var-decomp} allows us to decompose the variance into two
terms that we can interpret. First, note that the expression
\(\V[\E[Y|X_{\vu}]]\) first averages out all the variables \(\mX_{-\vu}\), then
computes the variance due only to \(\mX_{\vu}\). We may use this expression with
a singleton index set \(\vu=\{i\}\) to define the \emph{first-order sensitivity
  index}

\begin{equation}\label{eq:sobol-first}
  \underline{\tau}_{\{i\}}^2 = \frac{\V[\E[Y|X_i]]}{\V[Y]}.
\end{equation}

\noindent \Cref{eq:sobol-first} is bounded between \([0,1]\), and enables us to
rank variables according to their importance. Practically
\(\underline{\tau}_{\{i\}}^2\) tells us how much variance reduction we could
expect if we were able to exactly freeze \(\mX_i\). We could use this information
to inform which variables we should better characterize, in order to reduce
variability.

Note that we should not use \(\underline{\tau}_{\{i\}}^2\) to perform dimension
reduction; the first-order index does not account for interactions with other
variables, so we may miss some important cross terms. Instead, we may consider
the \emph{total-order sensitivity index}

\begin{equation}\label{eq:sobol-total}
  \overline{\tau}_{\{i\}}^2 = \frac{\E[\V[Y|X_{-\{i\}}]]}{\V[Y]} = %
  1 - \frac{\V[\E[Y|X_{-\{i\}}]]}{\V[Y]}.
\end{equation}

\noindent \Cref{eq:sobol-total} is also bounded between \([0,1]\), and accounts
for interactions between \(X_i\) and all other variables. We may interpret
\eqref{eq:sobol-total} in at least two ways: As the variability in \(Y\) due to
\(X_i\), averaged over all other inputs (middle expression), or as the complement
of the variability arising from all the variables excluding \(X_i\) (right
expression). If \(\overline{\tau}_{\{i\}}^2\) is zero or small, we can be
confident that \(X_i\) contributes little to the variablility in \(Y\), neither
through first-order nor interaction effects.

In summary: The notation for the sensitivity indices is suggestive of their use.
The first-order index \(\underline{\tau}_{\{i\}}^2\) can be thought of as a
lower bound; if it is large, then we can be confident that \(X_i\) is important.
In contrast the total-order index \(\overline{\tau}_{\{i\}}^2\) is an upper
bound; if it is small, then we can be confident that \(X_i\) is unimportant.

Now an example of a Sobol' index computation: A very common test case for
demonstrating Sobol' indices is the so-called \emph{Ishigami
  function},\cite{ishigami1990importance} given in \eqref{eq:ishigami}.

\begin{equation*} \begin{aligned} \label{eq:ishigami}
  f(\vx) &= \sin(x_1) + 7\sin^2(x_2) + 0.1 x_3^4\sin(x_1) \\
  x_1, x_2, x_3 &\sim U[-\pi, \pi], \\
  \underline{\tau}_{\{3\}}^2 &= 0\%, \\
  \overline{\tau}_{\{3\}}^2 &= 24.4\%
\end{aligned} \end{equation*}

\noindent Above, Sobol' indices for $x_3$ are reported; note that $x_3$ enters
as a scaling factor on $\sin(x_1)$. The first-order Sobol' index for $x_3$
indicates that this variable is unimportant in a first-order sense; this is
because we first average over all other variables ($\sin(\cdot)$ averages to
zero as it is anti-symmetric) before computing the variance contributed. The
total-order index tells a different story -- here $x_3$ contributes
$\approx24\%$ of the total variance, in cooperation with the other inputs. This
example helps to drive home why total-order indices should be used for dimension
reduction.

What would we get for total Sobol' indices on the pipe flow problem? The results
Table \ref{tab:pipe-sobol} suggest that the roughness lengthscale $\epsilon$
alone is sufficient to capture the vast majority of variance in the output qoi.

\begin{table}[!ht]
  \centering
  \begin{tabular}{@{}llllll@{}}
   & $\rho$ & $U$ & $d$ & $\mu$ & $\epsilon$\\
  \hline
  $\overline{\tau}^2$ & 0\% & 0\% & 6\% & 0\% & 94\% \\
  \end{tabular}
  \caption{Total Sobol' indices for pipe flow problem. The results suggest that,
    for capturing variance in $f$, the roughness $\epsilon$ alone is essentially
    the only relevant variable. Of course, this pre-supposes that variance is
    the `correct' measure to consider. The distributions for the inputs are
    chosen such that fully turbulent flow conditions are emphasized; this
    accounts for why roughness dominates in this setting.}
  \label{tab:pipe-sobol}
\end{table}

Sobol' indices seem to be useful for dimension reduction, but let's consider
their computational expense for a moment. Recall that Sobol' indices are defined
in terms of variance in the output qoi. Variance is defined in terms of an
expectation. Expectations are subject to the curse of dimensionality.

\bigskip
So is Sobol' itself cursed by dimensionality?

\bigskip
Many of the early applications of Sobol' indices seem to be on relatively cheap
models for financial and ecological applications.\cite{saltelli2004sensitivity}
In these cases -- where functions evaluate in fractions of a second -- only
large dimensionality would be crippling.\footnote{Of course, we've seen that
  modest numbers like $20$ can be `large'!} More recently, numerical analysts
have developed quadrature- and polynomial approximation-based approaches to
estimating Sobol' indices.\cite{sudret2008global} These approaches extend the
applicability of Sobol' indices to more complex simulations with higher
computational cost. Of course, many of these techniques are themselves cursed by
dimensionality, so while they extend the range of dimensionality we can treat,
they do not `solve' the underlying issue.

Within the framework of subset reduction, we are implicitly assuming that a
reasonable reduction exists within the variables `as stated'. Subset reduction
could not, for instance, find a change of variables that reduced the problem to
one-dimensional. Next, we'll consider an alternative framework for input-space
dimension reduction that allows for transformation of the input space, hence
generalizing the kind of low-dimensional structure we can seek.

\subsection{Subspace Reduction}
% -------------------------
In the \emph{subspace reduction} perspective, we seek low dimensional structure
of the form $\mW_A\in\R{d\times k}$ such that

\begin{equation} \label{eq:ridge}
  f(\vx) = g(\mW_A^{\top}\vx).
\end{equation}

\noindent In words, we are looking for a set of \emph{active directions} $\mW_A$
within the input space that capture the variability of $f$. These can be thought
of additionally in terms of their complementary \emph{inactive directions}
$\mW_I$, which do not contribute to changes in $f$. Note that in this context,
it is somewhat more difficult to `freeze' the inactive variables and vary the
active variables -- for any value of $\v\xi_A = \mW_A^{\top}\vx$, there is an
infinite family of $\vx$ that produce the same value of $\v\xi_A$. This becomes
problematic in the case where condition \ref{eq:ridge} only approximately
holds.\cite{constantine2015} We will defer the discussion of applying subspace
reduction to cited works, and instead focus on the intuition behind existing
techniques, and eventually point to a dimension reduction strategy that is not
itself cursed.

\subsubsection{Principal component analysis}
Before jumping straight to subspace reduction, we first review \emph{principal
  component analysis} (PCA), as it will aid in understanding what comes next.
PCA is a classical idea from statistics that helps to reduce dimensionality in
the generic-space setting. The high-level idea is to find a set of $k$
directions $\mW_A\in\R{k\times d}$ that capture a desired fraction of the
`variability' in the data $\mX\in\R{n\times d}$. This idea is visually depicted
in Figure \ref{fig:pca}.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.65\textwidth]{../../images/pca3}
  \caption{Visual example of PCA. Here we consider 2-dimensional data, and seek
    a 1-dimensional representation that captures the majority of the
    `variability' in the data. The dashed line represents the computed
    direction, and every data point is projected along this single direction to
    form a 1-dimensional summary of the 2-dimensional data.}
  \label{fig:pca}
\end{figure}

The vague concept of variability is made precise for PCA in terms of variance;
the PCA directions are computed by finding those directions $\mW_A$ that capture
the largest fraction of the variance. One can perform a PCA by first
centralizing the data $\mX' = \mX - \overline{\mX}$ by its sample mean
$\overline{\mX}$, and computing a singular value decomposition of these data

\begin{equation*}
  \mX' = \mU\m\Sigma\mW^{\top}.
\end{equation*}

\noindent With this representation, the entries of the diagonal matrix
$\m\Sigma$ represent the standard deviations of the data along the different PCA
directions. One can select the first $k$ directions $\mV_K$ -- ordered by the
descending singular values $\sigma_1 \geq \sigma_2 \geq \dots$ -- in order to
capture a desired fraction of the total variability $\text{Trace}(\m\Sigma)$. We
may then project the data to form a k-dimensional representation via

\begin{equation*}
  \mY = \mX'\mV_k = \mU\m\Sigma_k.
\end{equation*}

PCA is itself useful for data analysis in generic-space, but how can it be used
for the input-space setting? In other words, how do we connect $f$ to its inputs
with PCA? This linkage is formalized in the \emph{active subspace}.

\subsubsection{Active subspaces}
We can think of the \emph{active subspace} as an object arising from a PCA on
gradient data; in fact, an early formulation of the active subspace was exactly
posed as such.\cite{russi2010uncertainty} Formally, the active
subspace\cite{constantine2015} is defined in terms of a particular matrix

\begin{equation} \begin{aligned} \label{eq:c-exact}
    \mC &= \E[\nabla_{\vx}f\nabla_{\vx}f^{\top}] \\
        &= \mU\m\Sigma\mV^{\top},
\end{aligned} \end{equation}

\noindent where $\mC$ is broken into an SVD as above.\footnote{Usually we think
  of $\mC$ in its eigendecomposition; since $\mC$ is symmetric semi-positive
  definite, its eigendecomposition and SVD are equivalent.} In practice, we
cannot compute \eqref{eq:c-exact} exactly; instead, we may carry out a Monte
Carlo approximation

\begin{equation} \begin{aligned} \label{eq:c-monte-carlo}
  \hat{\mC} &\approx \frac{1}{n}\sum_{i=1}^n \nabla_{\vx}f_i\nabla_{\vx}f^{\top}_i \\
            &= \hat{\mU}\hat{\m\Sigma}\hat{\mV}^{\top}.
\end{aligned} \end{equation}

\noindent We can make the linkage to PCA more explicit by considering our data
matrix as $\mX = [\nabla_{\vx}f(\mX_1), \dots, \nabla_{\vx}f(\mX_n)]$ and
performing an SVD on this gradient data.\footnote{Though in this case we do not
  centralize: In PCA we are not interested in studying the mean trend, while in
  computing the active subspace, the mean gradient would represent a direction
  of large average change in the function -- something we would very much like
  to capture. Minyong Lee investigated some of these sorts of modifications in
  his PhD thesis.\cite{lee2017prediction}} This operation is equivalent to
computing and decomposing the $\hat{C}$ matrix above.

While PCA gave us a set of directions $\mV$ that captured variability in the
generic-space of the data, the set of directions $\mV$ arising from the $\mC$
matrix describe directions in the input-space that correspond to large changes
-- on average -- in the output qoi. Figure \ref{fig:as-contour} gives an
example, where movement along the inactive direction produce no change in the
qoi, while movement along the active direction accounts for all variability. In
practice, the inactive directions usually contribute nonzero change -- we must
select a number of active directions $k$ such that the inactive contributions
are negligible.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.65\textwidth]{../../images/as_contour1}
  \caption{Illustration of the active subspace.}
  \label{fig:as-contour}
\end{figure}

One way to select the number of active directions is based on \emph{eigenvalue
  gaps}.\footnote{Recall here that the SVD is closely related to the
  eigendecomposition; the singular values and eigenvalues are equivalent.} A
classic analysis bounds the subspace error -- the angle\footnote{where the
  precise angle is very carefully defined....} between the approximated
$\cR(\hat{\mV}_k)$ and true $\cR(\mV_k)$ subspaces -- in terms of the eigenvalue
gap $\delta = \sigma_k - \sigma_{k+1}$.\cite{davis1970rotation} The angle
$\m\Theta_0$ between true and approximated subspaces is thus bounded

\begin{equation} \label{eq:sine-bound}
  \|\sin(\m\Theta_0)\| \leq \frac{\|\mR\|}{\delta},
\end{equation}

\noindent where the angle $\m\Theta_0$ is expressed as a transformation matrix,
and the matrix $\mR$ measures the difference between $\hat{\mV_k}$ and $\mV_k$
on a lower-dimensional space. \Cref{eq:sine-bound} shows that a larger
eigenvalue gap implies a better-approximated subspace. For such accuracy
considerations, it is prudent to choose $k$ to correspond to the largest
eigenvalue gap.

However, while eigenvalue gaps signal a well-approximated space, it is a
separate question to ask whether the inactive directions contribute a meaningful
amount of variability to the qoi. This is more challenging to address.
Constantine\cite{constantine2015} shows that the eigenvalues $\sigma_j$
represent the average directional-derivative along direction $\vv_j$. While a
small $\sigma_j$ corresponds to small \textbf{average} change, large changes can
still occur over a small distance, which can be hidden by this average measure.

The most effective way to check that an active subspace is appropriate is
through visual inspection. In the case that $k$ is small enough to visualize
($k=1$, possibly even $k=2$), one can construct a \emph{sufficient summary
  plot}\cite{cook2009regression} by plotting the response $f_i$ against the
reduced coordinates $\mV_{A}^{\top}\vx_i$. If the data `collapse' to a single
curve, then the chosen active subspace can be justified. The example below will
illustrate this idea.

Returning to the pipe flow example, we approximate the $\hat{\mC}$ matrix via
Monte Carlo and compute its eigenvalue gaps (Tab. \ref{tab:as-pipe-gaps}). The
results suggest a one-dimensional active subspace -- we construct a
one-dimensional summary plot, which well-captures the trend in the data (Fig.
\ref{fig:as-pipe-summary-nat}).

\begin{table}[!ht]
  \centering
  \begin{tabular}{@{}ll@{}}
    \hline
    Gap Fraction & Dimension\\
    \hline
    0.9892887 & 1\\
    \hline
    0.0053363 & 2\\
    \hline
    0.0000129 & 3\\
    \hline
    0.0000000 & 4\\
    \hline
  \end{tabular}
  \caption{Eigenvalue gaps $\delta = \sigma_j - \sigma_{j+1}$ for the pipe flow
    problem, normalized by the eigenvalue sum $\sum \sigma_j$. Note that the
    gaps imply that $k=1$ is a good choice for subspace accuracy considerations.
    We will further probe this choice in Figure \ref{fig:as-pipe-summary-nat}.}
  \label{tab:as-pipe-gaps}
\end{table}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.65\textwidth]{../../images/as_summary_nat}
  \caption{Sufficient summary plot for pipe flow problem. Note that the data
    nearly collapse to a single curve; this is a pragmatic choice of dimension
    reduction.}
  \label{fig:as-pipe-summary-nat}
\end{figure}
\clearpage

At this point, we should ask the same question as we asked of Sobol'; the active
subspace is defined in terms of an expectation -- is it cursed by
dimensionality? The answer in this case is \textbf{no}, and the reasoning points
to a concept we've introduced earlier.

Holodnak et al.\cite{holodnak2018probabilistic} derived a probabilistic bound on
the number of gradient samples $n$ required to estimate the active subspace.
Their bound is

\begin{equation}
  n \geq \frac{C}{\epsilon^2}\log\left(\frac{4}{p}\text{intdim}(\mC)\right),
\end{equation}

\noindent where $n$ is the required number of samples, $\epsilon$ is the desired
error tolerance, $p$ is the desired probability of failure in approximation
(usually small), and $\text{intdim}(\mC)$ is the \emph{intrinsic dimensionality}
of the $\mC$ matrix, given by

\begin{equation} \label{eq:int-dim}
  \text{intdim}(\mC) = \frac{\text{trace}(\mC)}{\|\mC\|_2} \geq 1.
\end{equation}

\noindent \Cref{eq:int-dim} is a continuous version of what we might intuitively
think of as `intrinsic dimensionality'. Suppose there were only one nonzero
eigenvalue $\sigma_1$; then the trace and spectral norm of $\mC$ would be the
same, and $\text{intdim}(\mC)=1$. Suppose there were two nonzero eigenvalues of
equal value $\sigma_1=\sigma_2$; then
$\text{intdim}(\mC)=\frac{2\sigma}{\sigma}=2$. \Cref{eq:int-dim} automatically
interpolates between the more common case of unequal importance among
directions.

The punchline is that approximating the active subspace scales not with the
\emph{ambient dimension} $d$, but rather the underlying intrinsic dimensionality
$k$. This can be especially relevant when we can refine our parameterization of
a problem, as is often done in aerodynamic shape design.\cite{economon2016su2}
It seems that intrinsic dimensionality is important for practical reasons, but
where does this low-dimensional structure come from?

\section{Intrinsic Dimensionality}
% --------------------------------------------------

\subsection{The importance of vague concepts}
% -------------------------
Mosteller and Tukey\cite{mosteller1977data} describe in their ``second course in
statistics'' the importance of \emph{vague concepts}. They note that students
learn both about `variability' of data, and that they can numerically describe
this vague concept in different ways, say standard deviation and innerquartile
range. Further, they note that both the vague notion and different
operationalizations are \textbf{useful} -- different approaches are well-suited
for different contexts, and disagreement between methods is an opportunity for
further discovery.

I believe the same notion is useful in the area of dimension reduction. There
are many different ways we can think about low-dimensional structure; I propose
we use the term \emph{intrinsic dimensionality} to describe the number of
dimensions implied by this vague concept. We have already seen various
implementations of this idea:

\bigskip\noindent The Sobol' indices allowed us to rank variables, and enabled
us to fix some inputs, continuing our study in an \textbf{active set} of
variables, chosen from the coordinates of the input-space.

\bigskip\noindent Holodnak, Ipsen, and Smith introduced a particular notion of
intrinsic dimensionality in the context of active subspaces, which we studied
above. What we'll call the \textbf{HIS dimension} describes intrinsic
dimensionality in terms of the active subspace -- it is framed in terms of
important directions within input-space.

\bigskip\noindent \textbf{Low-rank} is a concept that appears in linear
algebra.[TODO Citation needed]

\bigskip\noindent \textbf{Sparsity} is a concept that appears in the compressed
sensing literature.\cite{candes2008introduction} It describes signals in terms
of a linear combination of a small number of basis functions, chosen from some
reference basis for the output-space.

\bigskip
I am certain there are yet more operationalizations of intrinsic dimensionality.
But there is one more that gets relatively little attention outside the fluid
mechanics community.

\subsection{Dimensional Analysis}
% -------------------------
\textcolor{red}{TODO:} Buckingham pi

\section{Annotated Bibliography}
% --------------------------------------------------
There is quite a bit more to the story of dimensionality than what I've
presented above. The last bit of value I'd like to add here is a set of
suggestions on what to read next.

\textcolor{red}{TODO:} Annotate the entries!

\bibliographystyle{plain}
\bibliography{bibtex_database}

\end{document}
